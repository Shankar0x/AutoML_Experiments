{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd52iDu7FpJy",
        "outputId": "3a1b7ce2-4510-402f-b29d-ebf1287cdd96"
      },
      "id": "Qd52iDu7FpJy",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6237196",
      "metadata": {
        "id": "c6237196"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Certificates/Colab Notebooks/Adanet/3_channel_3_class_meregd.csv')\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True) \n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "jBja3lTVGG7d",
        "outputId": "d830165a-930d-48d1-f039-05dcec6e3df0"
      },
      "id": "jBja3lTVGG7d",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         mean_r      std_r  min_r  max_r       var_r      mean_g      std_g  \\\n",
              "0    234.351562  19.891146  144.0  248.0  395.657654  171.654297  28.922405   \n",
              "1    240.658203  13.831165  160.0  248.0  191.301147  190.408203  29.130186   \n",
              "2    242.902344  11.503322  160.0  248.0  132.326401  198.523438  29.175911   \n",
              "3    239.787109  14.744755  152.0  248.0  217.407806  186.863281  29.279314   \n",
              "4    246.402344   6.019032  176.0  248.0   36.228745  224.769531  24.001497   \n",
              "..          ...        ...    ...    ...         ...         ...        ...   \n",
              "216  237.068359  16.921211  120.0  248.0  286.327362  211.224609  27.665842   \n",
              "217  235.962891  17.905966  128.0  248.0  320.623627  208.779297  28.125036   \n",
              "218  234.740234  18.878902  120.0  248.0  356.412994  205.990234  28.555864   \n",
              "219  235.222656  18.452356  120.0  248.0  340.489502  206.657227  28.295830   \n",
              "220  234.589844  18.823494  120.0  248.0  354.323975  205.105469  28.417585   \n",
              "\n",
              "     min_g  max_g       var_g  ...  max_a       var_a     mean_b1     std_b1  \\\n",
              "0     88.0  252.0  836.505493  ...  184.0  195.336441  151.947266  13.830170   \n",
              "1    104.0  252.0  848.567749  ...  181.0  158.647980  150.912598  13.704334   \n",
              "2    108.0  252.0  851.233826  ...  179.0  142.217026  150.710449  13.569794   \n",
              "3     96.0  252.0  857.278198  ...  181.0  165.136703  151.036865  13.638016   \n",
              "4    136.0  252.0  576.071899  ...  169.0   79.725052  148.221191  13.881875   \n",
              "..     ...    ...         ...  ...    ...         ...         ...        ...   \n",
              "216  100.0  252.0  765.398743  ...  182.0   93.706268  142.972900  15.570264   \n",
              "217   96.0  252.0  791.017700  ...  182.0   98.448662  143.115967  15.679187   \n",
              "218   92.0  252.0  815.437378  ...  186.0  103.035614  143.178223  15.713095   \n",
              "219   96.0  252.0  800.653992  ...  182.0  101.630920  143.342773  15.722949   \n",
              "220   92.0  252.0  807.559204  ...  184.0  104.150536  143.385986  15.706528   \n",
              "\n",
              "     min_b1  max_b1      var_b1  Target   temp  humidity  \n",
              "0     124.0   190.0  191.273590     0.0  40.88     43.90  \n",
              "1     125.0   197.0  187.808762     0.0  41.33     43.35  \n",
              "2     127.0   196.0  184.139297     0.0  42.15     41.65  \n",
              "3     126.0   194.0  185.995468     0.0  42.97     40.73  \n",
              "4     127.0   200.0  192.706451     0.0  43.31     40.39  \n",
              "..      ...     ...         ...     ...    ...       ...  \n",
              "216   118.0   195.0  242.433105     2.0  49.36     28.55  \n",
              "217   119.0   194.0  245.836899     2.0  49.42     28.64  \n",
              "218   118.0   193.0  246.901337     2.0  49.59     28.57  \n",
              "219   119.0   195.0  247.211121     2.0  49.76     28.42  \n",
              "220   119.0   193.0  246.695007     2.0  49.83     28.35  \n",
              "\n",
              "[221 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-836e675d-893b-44f4-8bdc-3f9a9f17b621\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_r</th>\n",
              "      <th>std_r</th>\n",
              "      <th>min_r</th>\n",
              "      <th>max_r</th>\n",
              "      <th>var_r</th>\n",
              "      <th>mean_g</th>\n",
              "      <th>std_g</th>\n",
              "      <th>min_g</th>\n",
              "      <th>max_g</th>\n",
              "      <th>var_g</th>\n",
              "      <th>...</th>\n",
              "      <th>max_a</th>\n",
              "      <th>var_a</th>\n",
              "      <th>mean_b1</th>\n",
              "      <th>std_b1</th>\n",
              "      <th>min_b1</th>\n",
              "      <th>max_b1</th>\n",
              "      <th>var_b1</th>\n",
              "      <th>Target</th>\n",
              "      <th>temp</th>\n",
              "      <th>humidity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>234.351562</td>\n",
              "      <td>19.891146</td>\n",
              "      <td>144.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>395.657654</td>\n",
              "      <td>171.654297</td>\n",
              "      <td>28.922405</td>\n",
              "      <td>88.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>836.505493</td>\n",
              "      <td>...</td>\n",
              "      <td>184.0</td>\n",
              "      <td>195.336441</td>\n",
              "      <td>151.947266</td>\n",
              "      <td>13.830170</td>\n",
              "      <td>124.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>191.273590</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.88</td>\n",
              "      <td>43.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>240.658203</td>\n",
              "      <td>13.831165</td>\n",
              "      <td>160.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>191.301147</td>\n",
              "      <td>190.408203</td>\n",
              "      <td>29.130186</td>\n",
              "      <td>104.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>848.567749</td>\n",
              "      <td>...</td>\n",
              "      <td>181.0</td>\n",
              "      <td>158.647980</td>\n",
              "      <td>150.912598</td>\n",
              "      <td>13.704334</td>\n",
              "      <td>125.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>187.808762</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41.33</td>\n",
              "      <td>43.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>242.902344</td>\n",
              "      <td>11.503322</td>\n",
              "      <td>160.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>132.326401</td>\n",
              "      <td>198.523438</td>\n",
              "      <td>29.175911</td>\n",
              "      <td>108.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>851.233826</td>\n",
              "      <td>...</td>\n",
              "      <td>179.0</td>\n",
              "      <td>142.217026</td>\n",
              "      <td>150.710449</td>\n",
              "      <td>13.569794</td>\n",
              "      <td>127.0</td>\n",
              "      <td>196.0</td>\n",
              "      <td>184.139297</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.15</td>\n",
              "      <td>41.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>239.787109</td>\n",
              "      <td>14.744755</td>\n",
              "      <td>152.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>217.407806</td>\n",
              "      <td>186.863281</td>\n",
              "      <td>29.279314</td>\n",
              "      <td>96.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>857.278198</td>\n",
              "      <td>...</td>\n",
              "      <td>181.0</td>\n",
              "      <td>165.136703</td>\n",
              "      <td>151.036865</td>\n",
              "      <td>13.638016</td>\n",
              "      <td>126.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>185.995468</td>\n",
              "      <td>0.0</td>\n",
              "      <td>42.97</td>\n",
              "      <td>40.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>246.402344</td>\n",
              "      <td>6.019032</td>\n",
              "      <td>176.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>36.228745</td>\n",
              "      <td>224.769531</td>\n",
              "      <td>24.001497</td>\n",
              "      <td>136.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>576.071899</td>\n",
              "      <td>...</td>\n",
              "      <td>169.0</td>\n",
              "      <td>79.725052</td>\n",
              "      <td>148.221191</td>\n",
              "      <td>13.881875</td>\n",
              "      <td>127.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>192.706451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.31</td>\n",
              "      <td>40.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>237.068359</td>\n",
              "      <td>16.921211</td>\n",
              "      <td>120.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>286.327362</td>\n",
              "      <td>211.224609</td>\n",
              "      <td>27.665842</td>\n",
              "      <td>100.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>765.398743</td>\n",
              "      <td>...</td>\n",
              "      <td>182.0</td>\n",
              "      <td>93.706268</td>\n",
              "      <td>142.972900</td>\n",
              "      <td>15.570264</td>\n",
              "      <td>118.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>242.433105</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.36</td>\n",
              "      <td>28.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>235.962891</td>\n",
              "      <td>17.905966</td>\n",
              "      <td>128.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>320.623627</td>\n",
              "      <td>208.779297</td>\n",
              "      <td>28.125036</td>\n",
              "      <td>96.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>791.017700</td>\n",
              "      <td>...</td>\n",
              "      <td>182.0</td>\n",
              "      <td>98.448662</td>\n",
              "      <td>143.115967</td>\n",
              "      <td>15.679187</td>\n",
              "      <td>119.0</td>\n",
              "      <td>194.0</td>\n",
              "      <td>245.836899</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.42</td>\n",
              "      <td>28.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>234.740234</td>\n",
              "      <td>18.878902</td>\n",
              "      <td>120.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>356.412994</td>\n",
              "      <td>205.990234</td>\n",
              "      <td>28.555864</td>\n",
              "      <td>92.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>815.437378</td>\n",
              "      <td>...</td>\n",
              "      <td>186.0</td>\n",
              "      <td>103.035614</td>\n",
              "      <td>143.178223</td>\n",
              "      <td>15.713095</td>\n",
              "      <td>118.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>246.901337</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.59</td>\n",
              "      <td>28.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>235.222656</td>\n",
              "      <td>18.452356</td>\n",
              "      <td>120.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>340.489502</td>\n",
              "      <td>206.657227</td>\n",
              "      <td>28.295830</td>\n",
              "      <td>96.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>800.653992</td>\n",
              "      <td>...</td>\n",
              "      <td>182.0</td>\n",
              "      <td>101.630920</td>\n",
              "      <td>143.342773</td>\n",
              "      <td>15.722949</td>\n",
              "      <td>119.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>247.211121</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.76</td>\n",
              "      <td>28.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>234.589844</td>\n",
              "      <td>18.823494</td>\n",
              "      <td>120.0</td>\n",
              "      <td>248.0</td>\n",
              "      <td>354.323975</td>\n",
              "      <td>205.105469</td>\n",
              "      <td>28.417585</td>\n",
              "      <td>92.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>807.559204</td>\n",
              "      <td>...</td>\n",
              "      <td>184.0</td>\n",
              "      <td>104.150536</td>\n",
              "      <td>143.385986</td>\n",
              "      <td>15.706528</td>\n",
              "      <td>119.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>246.695007</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.83</td>\n",
              "      <td>28.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>221 rows × 48 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-836e675d-893b-44f4-8bdc-3f9a9f17b621')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-836e675d-893b-44f4-8bdc-3f9a9f17b621 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-836e675d-893b-44f4-8bdc-3f9a9f17b621');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "306cb962",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306cb962",
        "outputId": "c3d8c0ac-2ace-4de8-fd04-a4df22257582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(221, 47) (221,)\n"
          ]
        }
      ],
      "source": [
        "y = df['Target']\n",
        "df.drop('Target', axis=1, inplace=True)\n",
        "x = df\n",
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3db1a466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3db1a466",
        "outputId": "ae994d81-ab17-417b-e0fe-7b1ed17c5084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(176, 47) (176,) (45, 47) (45,)\n"
          ]
        }
      ],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0a1b02c3",
      "metadata": {
        "id": "0a1b02c3"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2420b38e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2420b38e",
        "outputId": "ad9a1f3f-61bb-42ba-89da-e16a21ab585d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.95993092, 0.287324  , 0.65      , ..., 0.51709451, 0.40749022,\n",
              "        0.56776829],\n",
              "       [0.86491539, 0.58589643, 0.55      , ..., 0.59744312, 0.38960313,\n",
              "        0.65985889],\n",
              "       [0.99866912, 0.03182273, 0.9       , ..., 0.07977243, 0.84740078,\n",
              "        0.0571853 ],\n",
              "       ...,\n",
              "       [0.84937258, 0.59531395, 0.5       , ..., 0.81037419, 0.67411962,\n",
              "        0.24619384],\n",
              "       [0.28108562, 0.86076983, 0.15      , ..., 0.43455524, 0.24874231,\n",
              "        0.61047159],\n",
              "       [0.92585081, 0.40205065, 0.45      , ..., 0.71363325, 0.85969816,\n",
              "        0.04678797]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "88365c99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88365c99",
        "outputId": "34cda39c-77ae-49ad-d2d2-eb73a05ea564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 40)                1920      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                820       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 20)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 63        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,803\n",
            "Trainable params: 2,803\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "model = Sequential()\n",
        "model.add(tf.keras.Input(shape=(47,)))\n",
        "model.add(tf.keras.layers.Dense(40,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(20,activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "model.add(tf.keras.layers.Dense(3,activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6627bd24",
      "metadata": {
        "id": "6627bd24"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f68aeb17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f68aeb17",
        "outputId": "ff4eda66-37b5-4934-8612-9a3dccd91ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 57ms/step - loss: 1.1223 - accuracy: 0.3643 - val_loss: 1.1278 - val_accuracy: 0.3056\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.0827 - accuracy: 0.3786 - val_loss: 1.0866 - val_accuracy: 0.3611\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 1.0831 - accuracy: 0.4000 - val_loss: 1.0526 - val_accuracy: 0.5556\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.0477 - accuracy: 0.4643 - val_loss: 1.0282 - val_accuracy: 0.5833\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 1.0551 - accuracy: 0.4500 - val_loss: 1.0123 - val_accuracy: 0.6111\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 1.0291 - accuracy: 0.4500 - val_loss: 1.0002 - val_accuracy: 0.7222\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.9983 - accuracy: 0.5643 - val_loss: 0.9906 - val_accuracy: 0.6944\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9977 - accuracy: 0.4857 - val_loss: 0.9843 - val_accuracy: 0.6944\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 12ms/step - loss: 0.9951 - accuracy: 0.5000 - val_loss: 0.9789 - val_accuracy: 0.5833\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9956 - accuracy: 0.5071 - val_loss: 0.9752 - val_accuracy: 0.5556\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.9294 - accuracy: 0.6214 - val_loss: 0.9585 - val_accuracy: 0.5833\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.9518 - accuracy: 0.6000 - val_loss: 0.9420 - val_accuracy: 0.6111\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9243 - accuracy: 0.6000 - val_loss: 0.9304 - val_accuracy: 0.6111\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9426 - accuracy: 0.5929 - val_loss: 0.9206 - val_accuracy: 0.6111\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9143 - accuracy: 0.6143 - val_loss: 0.9195 - val_accuracy: 0.5833\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.9168 - accuracy: 0.5786 - val_loss: 0.9213 - val_accuracy: 0.5556\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.8978 - accuracy: 0.6143 - val_loss: 0.9092 - val_accuracy: 0.5278\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.8918 - accuracy: 0.6286 - val_loss: 0.8977 - val_accuracy: 0.5278\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.8813 - accuracy: 0.5786 - val_loss: 0.8853 - val_accuracy: 0.5278\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.8540 - accuracy: 0.6429 - val_loss: 0.8749 - val_accuracy: 0.5556\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.8588 - accuracy: 0.6000 - val_loss: 0.8586 - val_accuracy: 0.6111\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.8699 - accuracy: 0.5857 - val_loss: 0.8511 - val_accuracy: 0.6111\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.8527 - accuracy: 0.6500 - val_loss: 0.8486 - val_accuracy: 0.6111\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.8184 - accuracy: 0.7143 - val_loss: 0.8418 - val_accuracy: 0.6111\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.8336 - accuracy: 0.6429 - val_loss: 0.8288 - val_accuracy: 0.5833\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.8123 - accuracy: 0.6714 - val_loss: 0.8125 - val_accuracy: 0.6111\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.7853 - accuracy: 0.6857 - val_loss: 0.8035 - val_accuracy: 0.6111\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.7996 - accuracy: 0.6500 - val_loss: 0.7896 - val_accuracy: 0.6111\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7994 - accuracy: 0.6857 - val_loss: 0.7838 - val_accuracy: 0.6111\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.7820 - accuracy: 0.6500 - val_loss: 0.7776 - val_accuracy: 0.6111\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7957 - accuracy: 0.6286 - val_loss: 0.7707 - val_accuracy: 0.6111\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7750 - accuracy: 0.6786 - val_loss: 0.7660 - val_accuracy: 0.6111\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7459 - accuracy: 0.6857 - val_loss: 0.7596 - val_accuracy: 0.5833\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.7392 - accuracy: 0.6857 - val_loss: 0.7536 - val_accuracy: 0.5278\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.7588 - accuracy: 0.6286 - val_loss: 0.7437 - val_accuracy: 0.5278\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 47ms/step - loss: 0.7594 - accuracy: 0.6286 - val_loss: 0.7360 - val_accuracy: 0.5556\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 53ms/step - loss: 0.7032 - accuracy: 0.6857 - val_loss: 0.7250 - val_accuracy: 0.5833\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.7311 - accuracy: 0.6786 - val_loss: 0.7207 - val_accuracy: 0.5833\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.7440 - accuracy: 0.6357 - val_loss: 0.7034 - val_accuracy: 0.6389\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.6894 - accuracy: 0.6929 - val_loss: 0.6856 - val_accuracy: 0.6667\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 28ms/step - loss: 0.7137 - accuracy: 0.6500 - val_loss: 0.6742 - val_accuracy: 0.6944\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.6894 - accuracy: 0.6857 - val_loss: 0.6676 - val_accuracy: 0.7500\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.6716 - accuracy: 0.7000 - val_loss: 0.6747 - val_accuracy: 0.6667\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6683 - accuracy: 0.7071 - val_loss: 0.6585 - val_accuracy: 0.7222\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6689 - accuracy: 0.6500 - val_loss: 0.6429 - val_accuracy: 0.7500\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.6505 - accuracy: 0.6857 - val_loss: 0.6385 - val_accuracy: 0.7500\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 55ms/step - loss: 0.6305 - accuracy: 0.7571 - val_loss: 0.6262 - val_accuracy: 0.7500\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 48ms/step - loss: 0.6665 - accuracy: 0.6929 - val_loss: 0.6214 - val_accuracy: 0.7500\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.6675 - accuracy: 0.6357 - val_loss: 0.6137 - val_accuracy: 0.7500\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 40ms/step - loss: 0.6245 - accuracy: 0.7500 - val_loss: 0.6124 - val_accuracy: 0.7500\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 44ms/step - loss: 0.6268 - accuracy: 0.7286 - val_loss: 0.6005 - val_accuracy: 0.7500\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.6400 - accuracy: 0.7286 - val_loss: 0.5983 - val_accuracy: 0.7500\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5859 - accuracy: 0.7571 - val_loss: 0.6006 - val_accuracy: 0.7500\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5883 - accuracy: 0.7429 - val_loss: 0.5950 - val_accuracy: 0.7500\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6207 - accuracy: 0.7286 - val_loss: 0.5882 - val_accuracy: 0.7500\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.6019 - accuracy: 0.7214 - val_loss: 0.5720 - val_accuracy: 0.7500\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5978 - accuracy: 0.7214 - val_loss: 0.5632 - val_accuracy: 0.7500\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.6002 - accuracy: 0.6643 - val_loss: 0.5711 - val_accuracy: 0.7500\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 45ms/step - loss: 0.5513 - accuracy: 0.7286 - val_loss: 0.5728 - val_accuracy: 0.7222\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 43ms/step - loss: 0.5629 - accuracy: 0.7429 - val_loss: 0.5675 - val_accuracy: 0.7222\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 42ms/step - loss: 0.5493 - accuracy: 0.7286 - val_loss: 0.5461 - val_accuracy: 0.7222\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 36ms/step - loss: 0.5674 - accuracy: 0.7500 - val_loss: 0.5356 - val_accuracy: 0.7500\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 30ms/step - loss: 0.5778 - accuracy: 0.7071 - val_loss: 0.5388 - val_accuracy: 0.7500\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5402 - accuracy: 0.7500 - val_loss: 0.5383 - val_accuracy: 0.7500\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5402 - accuracy: 0.7286 - val_loss: 0.5255 - val_accuracy: 0.7222\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5585 - accuracy: 0.7214 - val_loss: 0.5138 - val_accuracy: 0.7222\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5549 - accuracy: 0.7714 - val_loss: 0.5084 - val_accuracy: 0.7222\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5348 - accuracy: 0.7500 - val_loss: 0.5001 - val_accuracy: 0.7222\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5080 - accuracy: 0.7857 - val_loss: 0.5041 - val_accuracy: 0.7500\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5344 - accuracy: 0.7571 - val_loss: 0.4980 - val_accuracy: 0.8056\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5353 - accuracy: 0.7714 - val_loss: 0.4853 - val_accuracy: 0.8056\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5196 - accuracy: 0.7643 - val_loss: 0.4736 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5361 - accuracy: 0.7000 - val_loss: 0.4747 - val_accuracy: 0.8056\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.5349 - accuracy: 0.7643 - val_loss: 0.4792 - val_accuracy: 0.7500\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.5375 - accuracy: 0.7714 - val_loss: 0.4785 - val_accuracy: 0.7500\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.5187 - accuracy: 0.7000 - val_loss: 0.4790 - val_accuracy: 0.7222\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4899 - accuracy: 0.7857 - val_loss: 0.4700 - val_accuracy: 0.7222\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.4902 - accuracy: 0.8071 - val_loss: 0.4647 - val_accuracy: 0.7222\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.4707 - accuracy: 0.7929 - val_loss: 0.4598 - val_accuracy: 0.7778\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.4674 - accuracy: 0.8143 - val_loss: 0.4537 - val_accuracy: 0.7778\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5112 - accuracy: 0.7429 - val_loss: 0.4472 - val_accuracy: 0.7778\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.5100 - accuracy: 0.7571 - val_loss: 0.4503 - val_accuracy: 0.7778\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.4709 - accuracy: 0.7857 - val_loss: 0.4366 - val_accuracy: 0.8056\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 24ms/step - loss: 0.4380 - accuracy: 0.8143 - val_loss: 0.4266 - val_accuracy: 0.8333\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.4741 - accuracy: 0.7857 - val_loss: 0.4134 - val_accuracy: 0.8333\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.4643 - accuracy: 0.7786 - val_loss: 0.4148 - val_accuracy: 0.8333\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.4661 - accuracy: 0.7857 - val_loss: 0.4158 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.4612 - accuracy: 0.8143 - val_loss: 0.4104 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 23ms/step - loss: 0.4523 - accuracy: 0.7857 - val_loss: 0.4100 - val_accuracy: 0.8333\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.4451 - accuracy: 0.8143 - val_loss: 0.4118 - val_accuracy: 0.8056\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.4598 - accuracy: 0.7786 - val_loss: 0.4120 - val_accuracy: 0.8333\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.4382 - accuracy: 0.8143 - val_loss: 0.4041 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.4607 - accuracy: 0.8071 - val_loss: 0.3963 - val_accuracy: 0.8333\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.4631 - accuracy: 0.7643 - val_loss: 0.3920 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.4245 - accuracy: 0.8500 - val_loss: 0.3932 - val_accuracy: 0.8611\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.4068 - accuracy: 0.8500 - val_loss: 0.3886 - val_accuracy: 0.8611\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.4244 - accuracy: 0.8143 - val_loss: 0.3738 - val_accuracy: 0.8333\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.4162 - accuracy: 0.8357 - val_loss: 0.3622 - val_accuracy: 0.8333\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 22ms/step - loss: 0.4403 - accuracy: 0.8214 - val_loss: 0.3716 - val_accuracy: 0.8611\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.4083 - accuracy: 0.8429 - val_loss: 0.3749 - val_accuracy: 0.8333\n",
            "2/2 - 0s - loss: 0.3254 - accuracy: 0.8889 - 41ms/epoch - 21ms/step\n",
            "Test loss:  0.32540908455848694\n",
            "Test accuracy:  0.8888888955116272\n"
          ]
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_split=0.2)\n",
        "test_score = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test loss: \", test_score[0])\n",
        "print(\"Test accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.trainable_variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTSWkC0368Ds",
        "outputId": "1293c2e6-86b6-443a-cf70-e114a08f1512"
      },
      "id": "XTSWkC0368Ds",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'dense/kernel:0' shape=(47, 40) dtype=float32, numpy=\n",
            "array([[ 0.12600282, -0.00135313,  0.15594317, ...,  0.02367239,\n",
            "         0.13093822, -0.16340701],\n",
            "       [-0.05428009, -0.0204363 ,  0.05891367, ..., -0.20016214,\n",
            "        -0.1144104 ,  0.09817024],\n",
            "       [-0.22036445,  0.20556258,  0.17142065, ...,  0.02698986,\n",
            "        -0.13604717,  0.17504156],\n",
            "       ...,\n",
            "       [ 0.04875286, -0.15716554, -0.24942043, ..., -0.04719989,\n",
            "        -0.1815308 ,  0.0235703 ],\n",
            "       [ 0.23390779, -0.12590724, -0.18199353, ..., -0.1562478 ,\n",
            "         0.22930604, -0.18324648],\n",
            "       [-0.00424905,  0.0217471 , -0.0603998 , ...,  0.12863041,\n",
            "        -0.16750632,  0.27824193]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(40,) dtype=float32, numpy=\n",
            "array([ 5.90957664e-02,  4.28126864e-02, -1.11413179e-02, -3.48014571e-03,\n",
            "       -6.40727952e-03,  3.90468836e-02,  1.13481944e-02,  3.37628610e-02,\n",
            "        3.45006287e-02,  3.23052667e-02,  8.67461599e-03,  0.00000000e+00,\n",
            "        1.71331614e-02, -1.24763427e-02,  2.17455043e-03, -3.88213270e-03,\n",
            "        0.00000000e+00,  2.40539443e-02,  3.01608890e-02, -7.86123564e-07,\n",
            "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.16723192e-02,\n",
            "        2.80948188e-02, -1.62965860e-02,  5.91506949e-04,  1.13435630e-02,\n",
            "       -1.32865738e-02,  1.19453715e-02, -2.51545059e-03,  5.32348156e-02,\n",
            "       -5.91883715e-03,  5.51835727e-03,  0.00000000e+00,  2.30997545e-03,\n",
            "        3.24699730e-02, -5.68097690e-03, -9.94533114e-03,  4.23208699e-02],\n",
            "      dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(40, 20) dtype=float32, numpy=\n",
            "array([[-3.59099120e-01,  2.15194508e-01, -1.47179782e-01,\n",
            "        -1.61650866e-01,  1.27720758e-01,  9.95339900e-02,\n",
            "        -8.55551958e-02,  2.61092454e-01, -4.38741475e-01,\n",
            "        -1.06650263e-01, -2.74253756e-01,  1.88488126e-01,\n",
            "        -4.41083312e-01,  2.83805579e-01,  3.75996739e-01,\n",
            "         2.81904846e-01, -2.10007519e-01, -1.94776475e-01,\n",
            "         2.37223059e-01, -4.99004394e-01],\n",
            "       [ 1.33310363e-01,  3.04415077e-01, -3.06908548e-01,\n",
            "         6.24152422e-02, -4.81895432e-02,  1.38275981e-01,\n",
            "        -2.55258739e-01,  1.66612908e-01, -1.05421230e-01,\n",
            "         1.51299670e-01,  7.05998391e-02, -3.60397808e-02,\n",
            "         3.36601003e-03,  3.36389303e-01,  4.23946083e-01,\n",
            "        -1.63980410e-01, -2.86258042e-01, -9.69688818e-02,\n",
            "        -1.90929249e-01,  9.52480659e-02],\n",
            "       [-3.58624682e-02,  2.12263331e-01, -1.31491452e-01,\n",
            "         2.69928485e-01, -1.23248532e-01, -1.67792127e-01,\n",
            "         4.57649529e-02,  2.98383236e-01, -4.60562930e-02,\n",
            "         1.66881740e-01,  2.19577789e-01, -1.26833227e-02,\n",
            "         1.69724926e-01,  4.84612696e-02, -2.00321436e-01,\n",
            "        -1.03046745e-01,  1.71475321e-01, -2.52252996e-01,\n",
            "         1.31687492e-01,  2.11822420e-01],\n",
            "       [-1.54853314e-01,  1.86869234e-01,  2.37957507e-01,\n",
            "         8.61964226e-02,  2.72396952e-01,  1.52632862e-03,\n",
            "        -5.28458059e-02, -1.23316124e-01, -8.73006955e-02,\n",
            "         1.24579072e-02, -1.11243995e-02,  2.34955877e-01,\n",
            "         2.64629692e-01,  1.22733191e-01,  5.31814396e-02,\n",
            "        -1.87623769e-01, -4.66269255e-03, -1.05194211e-01,\n",
            "         4.31157351e-02, -2.06969261e-01],\n",
            "       [ 5.77563681e-02, -1.37953043e-01, -3.03216577e-02,\n",
            "        -2.83872038e-01, -1.82836279e-01,  2.90591836e-01,\n",
            "         2.72467732e-03, -2.82877117e-01,  1.09000437e-01,\n",
            "         4.24530953e-02, -3.08180630e-01,  1.28736764e-01,\n",
            "        -2.85476059e-01, -6.24622889e-02,  3.29424143e-01,\n",
            "         1.33026481e-01,  2.73562402e-01, -1.55115992e-01,\n",
            "        -2.05276266e-01,  1.63624719e-01],\n",
            "       [-2.16425687e-01,  1.96799830e-01, -2.30423391e-01,\n",
            "         2.40574747e-01, -2.36823395e-01, -2.02819839e-01,\n",
            "         2.32141465e-01, -1.71839923e-01,  3.79428685e-01,\n",
            "         8.62616301e-02, -2.85983950e-01,  2.63545603e-01,\n",
            "         9.57126468e-02, -2.31442958e-01, -3.66076343e-02,\n",
            "        -1.44635588e-01, -8.85669142e-02,  1.20446898e-01,\n",
            "         1.86329961e-01,  1.20516688e-01],\n",
            "       [-3.31950009e-01,  3.41566175e-01, -1.16367221e-01,\n",
            "        -2.08486319e-02,  2.92398691e-01, -1.99983060e-01,\n",
            "        -1.78401813e-01, -3.01916957e-01, -2.07468837e-01,\n",
            "        -1.21031366e-01, -2.89463043e-01,  3.32067996e-01,\n",
            "         1.07460260e-01, -6.49913400e-02,  6.80744648e-02,\n",
            "         1.56278998e-01,  8.35135281e-02, -2.36138582e-01,\n",
            "         2.99149096e-01,  1.30229652e-01],\n",
            "       [ 1.18101530e-01, -3.95584255e-02,  2.35472053e-01,\n",
            "        -1.09492451e-01,  2.68230081e-01,  2.34550461e-01,\n",
            "         1.04704440e-01,  2.65074402e-01,  1.78933620e-01,\n",
            "        -4.95211184e-02,  2.78959990e-01,  7.12670153e-03,\n",
            "         1.78110689e-01,  2.07781613e-01,  7.92821497e-02,\n",
            "        -8.91922265e-02, -2.73874998e-01, -6.85931668e-02,\n",
            "         1.91867407e-02, -8.01050439e-02],\n",
            "       [ 1.31597206e-01,  2.19188824e-01, -2.71688282e-01,\n",
            "        -5.60453534e-03,  3.29950988e-01, -3.33059430e-02,\n",
            "        -1.89296395e-01, -8.49345140e-03,  2.29736671e-01,\n",
            "         7.92278945e-02,  3.02722573e-01, -1.33163020e-01,\n",
            "         1.08037457e-01,  5.47254048e-02,  1.16361506e-01,\n",
            "         2.05429941e-01, -4.36851382e-03, -3.08152646e-01,\n",
            "        -1.54633969e-01,  3.68920714e-01],\n",
            "       [-1.50395334e-01,  1.55834168e-01,  1.94545299e-01,\n",
            "         5.78656495e-02, -1.39533326e-01,  1.42876521e-01,\n",
            "         4.72929180e-02, -8.71671885e-02, -3.10202539e-01,\n",
            "         2.74490267e-01, -2.25375965e-01,  3.42467576e-01,\n",
            "        -2.97438174e-01, -1.97497755e-01, -1.71635468e-02,\n",
            "        -2.23465830e-01,  2.02929169e-01, -2.78786510e-01,\n",
            "        -2.69461721e-01, -7.56549090e-02],\n",
            "       [-1.31929040e-01,  3.66337508e-01,  5.00618517e-02,\n",
            "        -3.51789594e-03,  3.26551944e-01, -1.62950858e-01,\n",
            "         2.80023307e-01, -2.79340744e-01,  4.13732976e-01,\n",
            "         1.60380214e-01, -2.88443059e-01, -2.27332503e-01,\n",
            "        -4.14631963e-02, -1.79495350e-01,  2.86032647e-01,\n",
            "        -1.62461966e-01, -1.84793457e-01,  9.43906233e-02,\n",
            "         9.31834336e-03,  6.29706401e-03],\n",
            "       [-8.82030576e-02, -3.03516984e-01, -5.17669022e-02,\n",
            "         1.29964560e-01,  2.18706518e-01, -2.31531307e-01,\n",
            "         3.01597983e-01, -9.14030969e-02,  2.89552659e-01,\n",
            "        -1.80467471e-01,  4.78301048e-02,  2.37717062e-01,\n",
            "         1.42405331e-02, -2.57173896e-01,  1.42688841e-01,\n",
            "         2.45776743e-01, -6.38791919e-02, -4.94561195e-02,\n",
            "        -4.39790189e-02,  2.51507610e-01],\n",
            "       [-2.93639839e-01,  5.25820814e-02,  2.40391582e-01,\n",
            "        -1.56643167e-01,  1.34365931e-01,  2.33740598e-01,\n",
            "        -5.57499230e-02, -1.22335315e-01,  1.49661049e-01,\n",
            "        -2.98393101e-01,  1.38832266e-02,  7.60696903e-02,\n",
            "        -1.39136299e-01,  3.67184699e-01, -5.71565144e-02,\n",
            "        -1.18502244e-01, -2.48314887e-01,  1.16724506e-01,\n",
            "        -2.66107380e-01, -1.12123638e-01],\n",
            "       [ 1.10130608e-01, -1.21258087e-01,  2.47257948e-02,\n",
            "        -1.22867748e-01,  2.61967063e-01, -1.25086069e-01,\n",
            "        -2.86575764e-01, -2.06800744e-01,  8.73616189e-02,\n",
            "        -3.06811959e-01, -1.93478670e-02, -1.80775240e-01,\n",
            "        -2.89132208e-01, -1.32278591e-01, -7.53562152e-03,\n",
            "        -1.51734069e-01, -2.67713964e-02,  2.66081363e-01,\n",
            "         5.13548255e-02,  5.94486967e-02],\n",
            "       [ 1.39670298e-01,  5.77552840e-02,  2.93941498e-02,\n",
            "         1.62516624e-01, -1.57834083e-01, -2.79115170e-01,\n",
            "        -6.27647042e-02, -1.23211714e-02, -2.33361408e-01,\n",
            "        -1.71373576e-01,  2.55630434e-01,  1.80939108e-01,\n",
            "         3.13036263e-01,  9.09629613e-02, -1.69930518e-01,\n",
            "         4.44221795e-02, -2.21861303e-02,  2.22841322e-01,\n",
            "        -3.14284980e-01,  5.90183325e-02],\n",
            "       [ 1.80269465e-01, -2.20388457e-01, -5.88834584e-02,\n",
            "         3.29465270e-02, -2.05051675e-01,  1.31833360e-01,\n",
            "        -2.77595818e-01, -2.06811041e-01,  1.15757577e-01,\n",
            "        -2.33852267e-01, -2.39544153e-01, -1.22533299e-01,\n",
            "         1.69395253e-01,  1.44048229e-01, -2.77909547e-01,\n",
            "        -2.35106140e-01,  2.72544622e-02, -1.01055980e-01,\n",
            "        -1.46892369e-01, -1.98514596e-01],\n",
            "       [-2.09040344e-01,  1.37886494e-01, -3.01146746e-01,\n",
            "         1.39193386e-01,  1.64846301e-01,  2.68107682e-01,\n",
            "        -1.63351312e-01,  9.38589275e-02, -2.07585007e-01,\n",
            "        -1.78063363e-01, -1.46077812e-01,  9.75277722e-02,\n",
            "        -1.45172924e-01,  2.29207724e-01, -3.46001983e-03,\n",
            "        -3.08425546e-01, -5.90535998e-03,  1.52813345e-01,\n",
            "         2.62803167e-01,  2.47942537e-01],\n",
            "       [-2.55559474e-01,  2.06816606e-02, -5.72383404e-04,\n",
            "         2.68833905e-01, -1.75804898e-01, -4.26700652e-01,\n",
            "        -4.78922129e-02, -7.44562149e-02,  5.15045300e-02,\n",
            "        -8.38285219e-03, -1.57727405e-01, -1.15732096e-01,\n",
            "         4.37861145e-01, -1.62369803e-01,  8.18450004e-02,\n",
            "         3.50424945e-02, -1.40909672e-01, -7.07351277e-03,\n",
            "         1.58851162e-01,  2.90480286e-01],\n",
            "       [-2.80765146e-01,  9.84233022e-02, -2.75970995e-01,\n",
            "        -1.09889552e-01,  2.24614307e-01,  4.94894013e-02,\n",
            "        -1.97466761e-01,  3.01517576e-01,  6.08362481e-02,\n",
            "        -2.40100428e-01, -2.02238277e-01,  2.26357922e-01,\n",
            "         5.89553732e-03,  9.60167870e-02,  4.51223599e-03,\n",
            "        -1.40363589e-01, -1.40640885e-01,  2.66583472e-01,\n",
            "         2.06384331e-01, -2.44754016e-01],\n",
            "       [-3.06329817e-01, -4.21662591e-02, -1.48416787e-01,\n",
            "        -1.26157358e-01,  2.77436078e-01,  8.49187225e-02,\n",
            "         3.70157063e-02, -3.90991867e-02,  2.14471892e-01,\n",
            "        -1.11487634e-01,  1.18755147e-01,  3.05045456e-01,\n",
            "        -2.74158925e-01, -5.19148912e-03,  3.97963107e-01,\n",
            "         9.06358063e-02, -1.36832893e-02, -6.07661083e-02,\n",
            "         2.30904818e-01,  2.29674399e-01],\n",
            "       [-2.00865537e-01, -8.33131224e-02, -5.54153025e-02,\n",
            "         2.42195100e-01, -2.08739370e-01, -2.82909244e-01,\n",
            "        -2.08359167e-01,  1.75013423e-01, -9.78377312e-02,\n",
            "         2.37686038e-02, -2.58569390e-01, -4.94528711e-02,\n",
            "         1.54586256e-01,  8.79796743e-02, -1.84143409e-01,\n",
            "         1.51343286e-01,  2.04298943e-01,  1.94057614e-01,\n",
            "         2.43405730e-01, -1.53436407e-01],\n",
            "       [ 1.27073646e-01,  1.75962865e-01, -8.68636370e-03,\n",
            "        -1.87278762e-01, -2.92733014e-01,  8.49227011e-02,\n",
            "        -2.67880559e-02,  5.14721870e-02,  1.67959213e-01,\n",
            "        -1.42514676e-01, -1.31640807e-01,  8.70802104e-02,\n",
            "         7.62977898e-02, -1.02274328e-01, -1.87161073e-01,\n",
            "        -6.45096302e-02,  2.43744701e-01, -2.12942630e-01,\n",
            "        -2.77185440e-01, -2.65945852e-01],\n",
            "       [-1.65278256e-01, -1.07371807e-02,  2.81836092e-02,\n",
            "         2.92928070e-01,  3.09011251e-01, -8.37918669e-02,\n",
            "        -6.84934855e-02, -3.65205407e-02,  6.33077621e-02,\n",
            "        -1.48995429e-01,  1.65076196e-01, -1.96166351e-01,\n",
            "         1.04584187e-01,  1.00149781e-01,  3.06821615e-01,\n",
            "        -8.40550661e-02,  9.24900770e-02,  2.33720332e-01,\n",
            "        -1.68906689e-01,  2.90070742e-01],\n",
            "       [-7.71948844e-02, -1.32209910e-02, -1.28574893e-01,\n",
            "        -6.44623637e-02,  2.94604570e-01,  5.05559623e-01,\n",
            "        -1.00442767e-01, -2.44488686e-01, -1.01149138e-02,\n",
            "         1.46948278e-01, -1.31165832e-01,  4.23952341e-01,\n",
            "        -1.42574877e-01,  1.66010723e-01,  4.25366789e-01,\n",
            "        -3.59117985e-04,  1.81558043e-01, -4.62723263e-02,\n",
            "        -3.13738793e-01, -1.19012207e-01],\n",
            "       [ 3.17926109e-01, -2.36955389e-01, -2.29024217e-01,\n",
            "         1.14348471e-01, -2.83264905e-01,  2.65524983e-01,\n",
            "        -2.25310266e-01, -2.70805478e-01, -6.25336319e-02,\n",
            "        -2.84165125e-02, -8.68077874e-02,  2.58499295e-01,\n",
            "         1.71267539e-01,  3.50364774e-01,  2.56111860e-01,\n",
            "        -2.03555524e-02, -9.56910253e-02,  1.75144151e-01,\n",
            "        -1.96807683e-01, -5.86598776e-02],\n",
            "       [-1.71646595e-01, -3.42642397e-01,  2.44739562e-01,\n",
            "         1.35695755e-01,  9.69161391e-02,  2.53488958e-01,\n",
            "        -1.63329527e-01, -1.50254533e-01,  3.22837681e-01,\n",
            "         1.61204189e-01,  9.97267570e-03, -1.05046727e-01,\n",
            "        -4.81636561e-02, -2.53651202e-01, -2.44935408e-01,\n",
            "        -1.62233666e-01,  3.10695022e-01,  2.30726466e-01,\n",
            "        -5.99877052e-02, -1.67934969e-01],\n",
            "       [ 1.98239848e-01,  1.75644621e-01,  2.83575743e-01,\n",
            "        -2.83543617e-01,  1.34808227e-01, -2.34988704e-02,\n",
            "        -1.17268264e-01, -2.21698791e-01,  2.64437199e-01,\n",
            "        -2.09336966e-01, -2.89403826e-01, -2.75719434e-01,\n",
            "         1.81940079e-01,  4.34532017e-02, -3.27867210e-01,\n",
            "        -2.05117762e-02,  2.62590349e-02, -8.07149336e-02,\n",
            "        -2.44938299e-01,  3.02654475e-01],\n",
            "       [ 8.90571252e-02,  3.01765710e-01, -1.71975777e-01,\n",
            "        -1.37312412e-02,  2.73210347e-01,  1.71256021e-01,\n",
            "        -7.94060230e-02,  1.92263752e-01, -4.00329053e-01,\n",
            "         1.27506956e-01, -1.03830069e-01,  2.64107019e-01,\n",
            "        -1.16829008e-01,  1.84438199e-01, -7.92834535e-02,\n",
            "        -8.91789794e-03,  2.99205393e-01,  1.01456068e-01,\n",
            "         1.73336416e-01,  8.52056965e-02],\n",
            "       [-1.32551473e-02,  1.81721926e-01,  7.40214884e-02,\n",
            "        -1.37428701e-01,  1.78702727e-01, -5.56830056e-02,\n",
            "        -1.71305522e-01, -1.01101369e-01, -2.41678238e-01,\n",
            "        -1.68368015e-02, -1.49838567e-01, -3.10433179e-01,\n",
            "         2.18398690e-01,  6.01869561e-02,  4.18809894e-03,\n",
            "         1.67721570e-01,  2.03983158e-01, -5.88957816e-02,\n",
            "        -4.17251177e-02,  1.77970573e-01],\n",
            "       [-3.14726651e-01, -1.56572089e-02, -1.15343064e-01,\n",
            "        -3.41382921e-02,  1.30996764e-01,  1.43098801e-01,\n",
            "         7.76439011e-02, -1.17526287e-02,  2.53082693e-01,\n",
            "        -1.86949015e-01,  5.91659080e-03,  3.55339736e-01,\n",
            "         6.31950498e-02,  5.66277765e-02, -4.76727113e-02,\n",
            "         1.84746891e-01,  4.09380496e-02,  2.19593793e-01,\n",
            "        -5.21681309e-02, -8.67873132e-02],\n",
            "       [-2.72510899e-03, -2.80275732e-01, -1.79848105e-01,\n",
            "        -1.33042470e-01, -2.18679294e-01, -1.08307369e-01,\n",
            "         3.09619755e-01, -1.52713969e-01,  4.92249876e-01,\n",
            "         2.46840209e-01,  2.89169676e-03, -3.12959522e-01,\n",
            "         1.83565944e-01, -3.93825352e-01, -1.81154355e-01,\n",
            "        -2.54651070e-01, -3.16197455e-01,  1.80065840e-01,\n",
            "        -2.73552120e-01,  2.00689226e-01],\n",
            "       [ 1.39635697e-01,  2.32812807e-01,  1.48136020e-01,\n",
            "        -2.89268613e-01,  5.17659903e-01, -1.04314491e-01,\n",
            "        -2.68141776e-01,  1.21104963e-01,  2.22302720e-01,\n",
            "         2.00957894e-01, -1.74076796e-01,  2.66550332e-01,\n",
            "         2.51832843e-01,  4.35783595e-01, -2.05714881e-01,\n",
            "         1.52879149e-01,  2.56502658e-01,  1.86879933e-01,\n",
            "         1.57192096e-01, -5.94671024e-03],\n",
            "       [ 3.57541084e-01, -1.70431465e-01, -3.02115977e-02,\n",
            "        -2.94780105e-01,  1.10835336e-01,  4.33148928e-02,\n",
            "        -2.39037141e-01,  4.78214286e-02,  2.40964979e-01,\n",
            "         1.25430942e-01,  1.54647499e-01, -4.08200510e-02,\n",
            "         2.47049198e-01, -2.79981703e-01,  1.21663883e-01,\n",
            "        -1.08558461e-01, -2.20519707e-01, -1.91727489e-01,\n",
            "        -2.08405882e-01,  3.72199938e-02],\n",
            "       [ 3.25865060e-01, -2.52580911e-01, -1.31637111e-01,\n",
            "         2.45407909e-01, -2.12093413e-01, -2.01360956e-01,\n",
            "        -3.15451741e-01, -1.82883322e-01, -2.94179581e-02,\n",
            "        -3.35090846e-01, -2.77810365e-01,  2.37432912e-01,\n",
            "        -2.30045214e-01, -2.69595981e-01, -4.78605134e-03,\n",
            "         1.65510625e-01, -1.47108436e-02, -3.11328769e-01,\n",
            "        -1.32553235e-01, -3.74996848e-02],\n",
            "       [-1.46280706e-01, -2.73462296e-01,  1.12958729e-01,\n",
            "         1.71904862e-02,  1.07263923e-02, -7.81078786e-02,\n",
            "         2.12269455e-01, -2.96346366e-01,  6.98961318e-02,\n",
            "         1.15246803e-01,  2.45978922e-01,  9.69807208e-02,\n",
            "         1.62620515e-01, -1.91479072e-01,  1.61418051e-01,\n",
            "         1.67476237e-02, -2.03564897e-01,  5.02559245e-02,\n",
            "        -2.76410460e-01, -5.24455309e-02],\n",
            "       [-5.21143377e-02, -1.53072655e-01, -1.75534472e-01,\n",
            "        -1.62669748e-01, -2.31710169e-02,  3.43745857e-01,\n",
            "        -9.38811004e-02, -1.72137588e-01, -4.18038011e-01,\n",
            "         3.86466235e-02,  1.58039972e-01,  2.55293041e-01,\n",
            "        -3.92650813e-01,  2.02481091e-01, -1.09278575e-01,\n",
            "        -1.78030416e-01, -2.45507210e-01, -2.47943550e-01,\n",
            "         8.72039497e-02, -9.70828980e-02],\n",
            "       [-5.69798462e-02, -3.35793883e-01,  1.35708869e-01,\n",
            "        -2.61794776e-01, -4.02601629e-01,  3.45317066e-01,\n",
            "         2.63534814e-01,  1.19252853e-01,  1.41703352e-01,\n",
            "        -2.05326930e-01,  2.38957986e-01,  1.04322821e-01,\n",
            "        -2.09110364e-01, -1.96819492e-02,  6.97137937e-02,\n",
            "        -1.27603352e-01,  3.11212450e-01, -1.16256103e-01,\n",
            "        -2.75829703e-01,  1.71985999e-01],\n",
            "       [ 2.43377209e-01,  1.28265828e-01, -1.61691353e-01,\n",
            "        -1.00673735e-02, -3.96065265e-01, -3.11133415e-01,\n",
            "        -2.97682524e-01, -1.84877291e-01,  4.57304567e-01,\n",
            "        -1.37574509e-01,  2.43605196e-01, -4.95767780e-02,\n",
            "         9.68689099e-02, -3.13247740e-01, -3.72517705e-01,\n",
            "         6.75718486e-02,  1.54260755e-01,  1.95659786e-01,\n",
            "         1.01736307e-01,  1.51270747e-01],\n",
            "       [ 1.75536379e-01, -1.04213253e-01, -9.93957520e-02,\n",
            "        -8.33992213e-02, -7.94711560e-02, -2.78817862e-01,\n",
            "         1.39047861e-01, -1.85629427e-01, -1.23774879e-01,\n",
            "        -6.28646016e-02, -2.28194132e-01, -3.00527930e-01,\n",
            "        -3.33465487e-02,  1.41694965e-02, -2.53823400e-03,\n",
            "         8.94726217e-02, -3.05545032e-01, -2.03593850e-01,\n",
            "        -1.20971873e-01, -1.14946418e-01],\n",
            "       [-1.29881158e-01,  2.22072899e-01, -1.49887428e-01,\n",
            "        -1.02637053e-01,  3.04972559e-01, -4.72343177e-01,\n",
            "         1.53806508e-02, -2.98734665e-01,  1.47929087e-01,\n",
            "         2.53942907e-02, -2.36089140e-01,  3.15424800e-02,\n",
            "         2.74058223e-01, -4.08193357e-02,  3.12491972e-02,\n",
            "        -2.73386151e-01,  6.87487721e-02, -1.69462040e-01,\n",
            "        -2.57228971e-01, -2.76778400e-01]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(20,) dtype=float32, numpy=\n",
            "array([ 0.00236488,  0.0731855 ,  0.        ,  0.        ,  0.01594864,\n",
            "        0.01776655,  0.        , -0.01244837,  0.02111044, -0.04124214,\n",
            "        0.00158912,  0.02358943,  0.00346341,  0.0244381 ,  0.03926887,\n",
            "        0.        ,  0.        , -0.01235003, -0.00821223,  0.00164494],\n",
            "      dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(20, 3) dtype=float32, numpy=\n",
            "array([[ 0.5975219 , -0.31974787,  0.33002576],\n",
            "       [-0.21672553,  0.6222985 , -0.02141578],\n",
            "       [-0.46089694,  0.24933785,  0.03146243],\n",
            "       [ 0.46466994, -0.05317837,  0.04890633],\n",
            "       [-0.402343  ,  0.03097905, -0.3229222 ],\n",
            "       [-0.4662256 ,  0.05896521,  0.361406  ],\n",
            "       [ 0.05463052, -0.3274292 , -0.01274052],\n",
            "       [ 0.05579817, -0.22906674, -0.20461237],\n",
            "       [ 0.42106327,  0.12752096, -0.2671699 ],\n",
            "       [-0.1386318 , -0.31490302, -0.17700262],\n",
            "       [ 0.06640189, -0.11532216, -0.03982801],\n",
            "       [-0.44143298,  0.30357742,  0.20195803],\n",
            "       [ 0.29895326,  0.00920895, -0.661125  ],\n",
            "       [-0.54065585,  0.29498112,  0.17148574],\n",
            "       [-0.6486092 , -0.1635137 , -0.09352409],\n",
            "       [-0.37243038, -0.1884582 ,  0.13260043],\n",
            "       [ 0.2677831 ,  0.48780906,  0.21030295],\n",
            "       [-0.03094355,  0.08335865,  0.44138053],\n",
            "       [ 0.40326375,  0.34741595,  0.38017008],\n",
            "       [ 0.39478925, -0.08949345, -0.05230753]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.0024204 , -0.03346396,  0.03423899], dtype=float32)>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "Bjt6113iHGFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc66c16-0d5e-447c-c7b1-39a89730a2d9"
      },
      "id": "Bjt6113iHGFv",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzyxKDrwG_Ml",
        "outputId": "3c88336b-475f-4607-918e-12cd7340f627"
      },
      "id": "uzyxKDrwG_Ml",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 47)               3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (None, 40)               1925      \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWrap  (None, 20)               825       \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dropout (QuantizeWrap  (None, 20)               1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense_2 (QuantizeWrap  (None, 3)                68        \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,822\n",
            "Trainable params: 2,803\n",
            "Non-trainable params: 19\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_aware_model.fit(x_train, y_train, batch_size=32, epochs=1, validation_split=0.2)\n",
        "test_score = q_aware_model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"Test loss: \", test_score[0])\n",
        "print(\"Test accuracy: \", test_score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VytwZDo6HUT2",
        "outputId": "e4a25684-62f8-42a8-8f66-953f61009150"
      },
      "id": "VytwZDo6HUT2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 2s 69ms/step - loss: 0.4265 - accuracy: 0.8429 - val_loss: 0.3781 - val_accuracy: 0.8333\n",
            "2/2 - 0s - loss: 0.3227 - accuracy: 0.9333 - 29ms/epoch - 15ms/step\n",
            "Test loss:  0.32272839546203613\n",
            "Test accuracy:  0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    x_test, y_test, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
        "   x_test, y_test, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant test accuracy:', q_aware_model_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDdLhe2qHtrv",
        "outputId": "fe2cb41f-5a03-4eb3-9bc9-d4c54c67a789"
      },
      "id": "MDdLhe2qHtrv",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.8888888955116272\n",
            "Quant test accuracy: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9FxbbMNH4xo",
        "outputId": "bff34578-f5e5-4394-ce1d-4282d88ebda5"
      },
      "id": "t9FxbbMNH4xo",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3e1871e",
      "metadata": {
        "id": "d3e1871e"
      },
      "outputs": [],
      "source": [
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2084a564",
      "metadata": {
        "id": "2084a564"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(x_test).to_csv('x_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(y_test).to_csv('y_test.csv', index = False)"
      ],
      "metadata": {
        "id": "6K6heC7zIeCa"
      },
      "id": "6K6heC7zIeCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osS1Ac4GIpNT",
        "outputId": "bc648576-0872-4131-a0ca-ac7844069544"
      },
      "id": "osS1Ac4GIpNT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VD_mpq1VM-Fg"
      },
      "id": "VD_mpq1VM-Fg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}